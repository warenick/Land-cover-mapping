{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b8765a-636d-4e12-8906-147ef40fd46d",
   "metadata": {},
   "source": [
    "### Recreating the ResNet-110 experiment described in the [original paper](https://www.researchgate.net/publication/335844699_SEN12MS_-_A_Curated_D_of_Georeferenced_Multi-Spectral_Sentinel-12_Imagery_for_Deep_Learning_and_Data_Fusion)\n",
    "\n",
    "\n",
    "#### Input\n",
    "- 64 × 64 Sentinel-2 images from the summer subset\n",
    "- 10 bands: B2, B3, B4, B8, B5, B6, B7, B8a, B11, and B12.\n",
    "\n",
    "\n",
    "#### Label\n",
    "- Majority LCCS land use class from each of the 64 × 64 patches\n",
    "- 8 classes instead of 11: 20 and 25 combined; 30, 35, and 36 combined\n",
    "\n",
    "#### Training parameters\n",
    "- Categorical cross-entropy loss\n",
    "- Adam optimizer with 0.0005 starting learning rate\n",
    "- ReduceOnPlateau learning rate scheduler\n",
    "- Batch size: 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c50cd0bb-636c-4e52-a723-c083063a8d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import sen12ms_dataLoader as sen12ms\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "365cbd4c-3ac0-4623-af5e-f49cfc324338",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = pathlib.Path('/home/dubrovin/Projects/Data/SEN12MS/')\n",
    "SEASON = sen12ms.Seasons.SUMMER\n",
    "\n",
    "assert DATASET_PATH.is_dir(), 'Incorect location for the dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e99b7e-7cbb-4738-9137-24086b998117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" PyTotch wrapper for the dataloader provided by the dataset authors. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dataset = sen12ms.SEN12MSDataset(base_dir=DATASET_PATH)\n",
    "        \n",
    "        # get a dictionary {scene_id: patch_ids} for the whole season\n",
    "        season_ids = self._dataset.get_season_ids(season=SEASON)\n",
    "        \n",
    "        # flatten it into a list of tuples unique for each 64x64 patch\n",
    "        # (scene_id, patch_id, subpatch_idx)\n",
    "        self.subpatch_unique_ids = []\n",
    "        \n",
    "        for scene_id, patch_ids in season_ids.items():\n",
    "            for patch_id in patch_ids:\n",
    "                # there are 16 64x64 patches in 256x256 image\n",
    "                for i in range(16):\n",
    "                    self.subpatch_unique_ids.append((scene_id, patch_id, i))\n",
    "        \n",
    "        self.lc_bands = sen12ms.LCBands.landuse\n",
    "        self.s2_bands = [\n",
    "            sen12ms.S2Bands.B02,   # blue\n",
    "            sen12ms.S2Bands.B03,   # green\n",
    "            sen12ms.S2Bands.B04,   # red\n",
    "            sen12ms.S2Bands.B08,   # near-infrared\n",
    "            sen12ms.S2Bands.B05,   # red edge 1\n",
    "            sen12ms.S2Bands.B06,   # red edge 2\n",
    "            sen12ms.S2Bands.B07,   # red edge 3\n",
    "            sen12ms.S2Bands.B08A,  # red edge 4\n",
    "            sen12ms.S2Bands.B11,   # short-wave infrered 1\n",
    "            sen12ms.S2Bands.B12    # short-wave infrared 2\n",
    "        ]\n",
    "        \n",
    "        self.class_to_target_map = {\n",
    "            0: 0,  # turns out, some subpatches [i.e. idx=54361] have mode NODATA, 0\n",
    "            1: 0,\n",
    "            2: 1,\n",
    "            3: 2,\n",
    "            9: 3,\n",
    "            10: 4,\n",
    "            20: 5,\n",
    "            30: 6,\n",
    "            40: 7\n",
    "        }\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.subpatch_unique_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        scene, patch, subpatch = self.subpatch_unique_ids[idx]\n",
    "        s2, _ = self._dataset.get_patch(SEASON, scene, patch, self.s2_bands)\n",
    "        lc, _ = self._dataset.get_patch(SEASON, scene, patch, self.lc_bands)\n",
    "        \n",
    "        i = subpatch // 4  # row number of the 64x64 subpatch of the 256x256 image\n",
    "        j = subpatch % 4   # column number of the 64x64 subpatch\n",
    "        s2_subpatch = s2[:, i * 64:(i + 1) * 64, j * 64:(j + 1) * 64]\n",
    "        lc_subpatch = lc[:, i * 64:(i + 1) * 64, j * 64:(j + 1) * 64]\n",
    "        \n",
    "        image = s2_subpatch - s2_subpatch.mean(axis=(1, 2), keepdims=True)\n",
    "        image /= s2_subpatch.std(axis=(1, 2), keepdims=True)\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        # combine classes 20 and 25; 30, 35, and 36\n",
    "        lc_subpatch[lc_subpatch == 25] = 20\n",
    "        lc_subpatch[lc_subpatch == 35] = 30\n",
    "        lc_subpatch[lc_subpatch == 36] = 30\n",
    "        \n",
    "        # use the most common value as the label\n",
    "        values, counts = np.unique(lc_subpatch, return_counts=True)\n",
    "        mode = values[np.argmax(counts)]\n",
    "        label = self.class_to_target_map[mode]\n",
    "\n",
    "        return torch.tensor(image), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5814a25-b275-43c9-9245-53f100b0e662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet110(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet110, self).__init__()\n",
    "        \n",
    "        self.resnet = models.resnet101()\n",
    "        self.resnet.conv1 = nn.Conv2d(10, 64, 7, 2, 3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(2048, 8)\n",
    "        \n",
    "        # to transform the ResNet-101 to ResNet-110, add 3 extra bottleneck blocks\n",
    "        # each bottleneck block adds 3 layers, 101 + 3 * 3 = 110\n",
    "        for i in range(3):\n",
    "            self.resnet.layer3.add_module(f'extra_{i}', models.resnet.Bottleneck(1024, 256))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        dataset = Dataset()\n",
    "        n_val_examples = int(len(dataset) * 0.1)\n",
    "        splits = [len(dataset) - n_val_examples, n_val_examples]\n",
    "        self.train_data, self.val_data = torch.utils.data.random_split(dataset, splits)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        dataloader = torch.utils.data.DataLoader(self.train_data, batch_size=16, shuffle=True, num_workers=12, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataloader = torch.utils.data.DataLoader(self.val_data, batch_size=16, num_workers=12, pin_memory=True)\n",
    "        return dataloader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        pred = self(x)\n",
    "        loss = self.criterion(pred, y)\n",
    "        accuracy = self.accuracy(pred.softmax(dim=1), y)\n",
    "        \n",
    "        batch_dict = {\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "        }\n",
    "        \n",
    "        return batch_dict\n",
    "    \n",
    "    def training_epoch_end(self, train_step_outputs):\n",
    "        average_loss = torch.tensor([x['loss'] for x in train_step_outputs]).mean()\n",
    "        average_accuracy = torch.tensor([x['accuracy'] for x in train_step_outputs]).mean()\n",
    "        \n",
    "        # log to TebsorBoard\n",
    "        self.logger.experiment.add_scalar('Loss/train', average_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Accuracy/train', average_accuracy, self.current_epoch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_dict = self.training_step(batch, batch_idx)\n",
    "        return batch_dict\n",
    "    \n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        average_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n",
    "        average_accuracy = torch.tensor([x['accuracy'] for x in val_step_outputs]).mean()\n",
    "        \n",
    "        # log to TebsorBoard\n",
    "        self.logger.experiment.add_scalar('Loss/validation', average_loss, self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Accuracy/validation', average_accuracy, self.current_epoch)\n",
    "        \n",
    "        # log to the system for ReduceLROnPlateau and EarlyStopping / ModelCheckpoint\n",
    "        self.log('system/val_loss', average_loss)\n",
    "        self.log('system/val_acc', average_accuracy)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0005)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "        return {'optimizer': optimizer, 'scheduler': scheduler, 'monitor': 'system/val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd804db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = pl.callbacks.EarlyStopping(\n",
    "    monitor='system/val_loss',\n",
    "    patience=4,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "checkpoint_acc = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='system/val_acc',\n",
    "    mode='max',\n",
    "    every_n_val_epochs=1,\n",
    "    dirpath='./best_models/',\n",
    "    filename=r'resnet110_v0_val_acc={system/val_acc:.2f}',\n",
    "    auto_insert_metric_name=False,\n",
    "    save_weights_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85cff755-1143-43f1-8271-a56cbd31f3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model = ResNet110()\n",
    "logger = pl.loggers.TensorBoardLogger('runs', 'resnet110', default_hp_metric=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    gpus=1, \n",
    "    callbacks=[stop_early, checkpoint_acc],\n",
    "    profiler='simple',\n",
    "    num_sanity_val_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a18bf-828c-4514-bfcd-46a7504eb2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
